{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0818c835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshiya George\\AppData\\Local\\Temp\\ipykernel_17672\\2629574906.py:14: DtypeWarning: Columns (37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined CSV saved to: C:/Users/Akshiya George/OneDrive/Desktop/Data Science/Amazon\\amazon_india_combined_2015_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Folder containing your CSVs ---\n",
    "folder_path = \"C:/Users/Akshiya George/OneDrive/Desktop/Data Science/Amazon\"\n",
    "\n",
    "# --- List of years to combine ---\n",
    "years = range(2015, 2026)\n",
    "\n",
    "# --- Build full file paths ---\n",
    "csv_files = [os.path.join(folder_path, f\"amazon_india_{year}.csv\") for year in years]\n",
    "\n",
    "# --- Read and combine all CSVs ---\n",
    "combined_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# --- Save to a single CSV file ---\n",
    "output_path = os.path.join(folder_path, \"amazon_india_combined_2015_2025.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Combined CSV saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0cbf080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted customers with columns: ['customer_id', 'customer_state', 'customer_tier', 'customer_age_group']\n",
      "‚úÖ Loaded customers with 451,322 rows\n",
      "‚úÖ Loaded products with 2,004 rows\n",
      "‚úÖ Loaded time_dimension with 4,015 rows\n",
      "‚úÖ Loaded transactions with 1,127,400 rows\n",
      "‚ö†Ô∏è Index already exists: idx_order_date on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_customer_id on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_product_id on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_payment_method on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_order_year_month on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_return_status on transactions\n",
      "‚ö†Ô∏è Index already exists: idx_category_subcategory on products\n",
      "‚ö†Ô∏è Index already exists: idx_brand on products\n",
      "‚ö†Ô∏è Index already exists: idx_prime_eligible on products\n",
      "‚ö†Ô∏è Index already exists: idx_customer_location on customers\n",
      "‚ö†Ô∏è Index already exists: idx_customer_tier on customers\n",
      "‚ö†Ô∏è Index already exists: idx_spending_tier on customers\n",
      "‚ö†Ô∏è Index already exists: idx_year_month on time_dimension\n",
      "‚ö†Ô∏è Index already exists: idx_day_of_week on time_dimension\n",
      "‚ö†Ô∏è Index already exists: idx_is_weekend on time_dimension\n",
      "‚ö†Ô∏è Index already exists: idx_is_holiday on time_dimension\n",
      "üéâ All data from 2015‚Äì2025 loaded and indexed successfully.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Path to Combined CSV ---\n",
    "combined_path = \"C:/Users/Akshiya George/OneDrive/Desktop/Data Science/Amazon/amazon_india_combined_2015_2025.csv\"\n",
    "\n",
    "# --- Connect to MySQL ---\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"Akshiya13\",\n",
    "    database=\"amazon\",\n",
    "    allow_local_infile=True\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SET GLOBAL local_infile = 1;\")\n",
    "\n",
    "# --- Function to Save Temp CSV and Load into MySQL ---\n",
    "def load_table(df, table_name):\n",
    "    temp_path = f\"{table_name}_temp.csv\"\n",
    "    df.to_csv(temp_path, index=False)\n",
    "    query = f\"\"\"\n",
    "    LOAD DATA LOCAL INFILE '{temp_path.replace(\"\\\\\", \"/\")}'\n",
    "    INTO TABLE {table_name}\n",
    "    FIELDS TERMINATED BY ',' \n",
    "    ENCLOSED BY '\"'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    IGNORE 1 ROWS;\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "    os.remove(temp_path)\n",
    "    print(f\"‚úÖ Loaded {table_name} with {len(df):,} rows\")\n",
    "\n",
    "# --- Read and Clean Combined CSV ---\n",
    "df = pd.read_csv(\"C:/Users/Akshiya George/OneDrive/Desktop/Data Science/Amazon/amazon_india_combined_2015_2025.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "df = df.dropna(subset=['order_date'])\n",
    "\n",
    "# --- Extract Transactions ---\n",
    "transactions = df[[\n",
    "    'transaction_id', 'order_date', 'customer_id', 'product_id',\n",
    "    'original_price_inr', 'discount_percent', 'discounted_price_inr',\n",
    "    'quantity', 'subtotal_inr', 'delivery_charges', 'final_amount_inr',\n",
    "    'payment_method', 'payment_method_cleaned', 'delivery_days',\n",
    "    'delivery_type', 'is_prime_member', 'is_festival_sale', 'festival_name',\n",
    "    'customer_rating', 'return_status', 'order_month', 'order_year',\n",
    "    'order_quarter', 'dup_key', 'dup_count', 'dup_status', 'flag_for_review'\n",
    "]].drop_duplicates()\n",
    "\n",
    "# --- Extract Products ---\n",
    "products = df[[\n",
    "    'product_id', 'product_name', 'category', 'subcategory', 'brand',\n",
    "    'product_weight_kg', 'is_prime_eligible', 'product_rating'\n",
    "]].drop_duplicates()\n",
    "\n",
    "# --- Extract Customers ---\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Define expected columns\n",
    "expected_cols = ['customer_id', 'customer_name', 'customer_state', 'customer_tier', 'customer_points', 'customer_age_group']\n",
    "\n",
    "# Filter only existing columns\n",
    "valid_cols = [col for col in expected_cols if col in df.columns]\n",
    "\n",
    "# Extract and deduplicate\n",
    "customers = df[valid_cols].drop_duplicates()\n",
    "\n",
    "print(f\"‚úÖ Extracted customers with columns: {valid_cols}\")\n",
    "\n",
    "# --- Create Time Dimension ---\n",
    "time_dimension = df[['order_date']].drop_duplicates().copy()\n",
    "time_dimension['year'] = time_dimension['order_date'].dt.year\n",
    "time_dimension['month'] = time_dimension['order_date'].dt.month\n",
    "time_dimension['day'] = time_dimension['order_date'].dt.day\n",
    "time_dimension['quarter'] = time_dimension['order_date'].dt.quarter\n",
    "time_dimension['week'] = time_dimension['order_date'].dt.isocalendar().week\n",
    "time_dimension['day_of_week'] = time_dimension['order_date'].dt.day_name()\n",
    "time_dimension['is_weekend'] = time_dimension['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "time_dimension['is_holiday'] = df.groupby('order_date')['is_festival_sale'].max().reindex(time_dimension['order_date']).fillna(False).values\n",
    "time_dimension['holiday_name'] = df.groupby('order_date')['festival_name'].first().reindex(time_dimension['order_date']).fillna(\"\").values\n",
    "time_dimension.rename(columns={'order_date': 'date'}, inplace=True)\n",
    "\n",
    "# --- Load Tables into MySQL ---\n",
    "load_table(customers, \"customers\")\n",
    "load_table(products, \"products\")\n",
    "load_table(time_dimension, \"time_dimension\")\n",
    "load_table(transactions, \"transactions\")\n",
    "\n",
    "# --- Helper: Check if Index Exists ---\n",
    "def index_exists(cursor, table, index_name):\n",
    "    cursor.execute(f\"SHOW INDEX FROM {table} WHERE Key_name = '{index_name}';\")\n",
    "    result = cursor.fetchall()\n",
    "    return len(result) > 0\n",
    "\n",
    "# --- Index Creation Queries ---\n",
    "index_queries = [\n",
    "    (\"transactions\", \"idx_order_date\", \"CREATE INDEX idx_order_date ON transactions(order_date);\"),\n",
    "    (\"transactions\", \"idx_customer_id\", \"CREATE INDEX idx_customer_id ON transactions(customer_id);\"),\n",
    "    (\"transactions\", \"idx_product_id\", \"CREATE INDEX idx_product_id ON transactions(product_id);\"),\n",
    "    (\"transactions\", \"idx_payment_method\", \"CREATE INDEX idx_payment_method ON transactions(payment_method_cleaned);\"),\n",
    "    (\"transactions\", \"idx_order_year_month\", \"CREATE INDEX idx_order_year_month ON transactions(order_year, order_month);\"),\n",
    "    (\"transactions\", \"idx_return_status\", \"CREATE INDEX idx_return_status ON transactions(return_status);\"),\n",
    "    (\"products\", \"idx_category_subcategory\", \"CREATE INDEX idx_category_subcategory ON products(category, subcategory);\"),\n",
    "    (\"products\", \"idx_brand\", \"CREATE INDEX idx_brand ON products(brand);\"),\n",
    "    (\"products\", \"idx_prime_eligible\", \"CREATE INDEX idx_prime_eligible ON products(is_prime_eligible);\"),\n",
    "    (\"customers\", \"idx_customer_location\", \"CREATE INDEX idx_customer_location ON customers(customer_state);\"),\n",
    "    (\"customers\", \"idx_customer_tier\", \"CREATE INDEX idx_customer_tier ON customers(customer_tier);\"),\n",
    "    (\"customers\", \"idx_spending_tier\", \"CREATE INDEX idx_spending_tier ON customers(customer_points);\"),\n",
    "    (\"time_dimension\", \"idx_year_month\", \"CREATE INDEX idx_year_month ON time_dimension(year, month);\"),\n",
    "    (\"time_dimension\", \"idx_day_of_week\", \"CREATE INDEX idx_day_of_week ON time_dimension(day_of_week);\"),\n",
    "    (\"time_dimension\", \"idx_is_weekend\", \"CREATE INDEX idx_is_weekend ON time_dimension(is_weekend);\"),\n",
    "    (\"time_dimension\", \"idx_is_holiday\", \"CREATE INDEX idx_is_holiday ON time_dimension(is_holiday);\")\n",
    "]\n",
    "\n",
    "# --- Safe Index Creation ---\n",
    "for table, index_name, query in index_queries:\n",
    "    try:\n",
    "        if index_exists(cursor, table, index_name):\n",
    "            print(f\"‚ö†Ô∏è Index already exists: {index_name} on {table}\")\n",
    "        else:\n",
    "            cursor.execute(query)\n",
    "            print(f\"‚úÖ Index created: {index_name} on {table}\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"‚ùå Index creation failed for {index_name}: {err}\")\n",
    "\n",
    "# --- Close Connection ---\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"üéâ All data from 2015‚Äì2025 loaded and indexed successfully.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
